# ğŸ“Š **On Functional Competence of LLMs for Linguistic Disambiguation**

**Author(s)**: Raihan Kibria, Sheikh Intiser Uddin Dipta, Muhammad Abdullah Adnan  
**Published in**: CoNLL  
**Year**: 2024


## ğŸ“ **Abstract**

We study some Large Language Models to explore their deficiencies in resolving sense ambiguities. In this connection, we evaluate their performance on well-known word sense disambiguation datasets. Word Sense Disambiguation (WSD) has been a long-standing NLP problem, which has given rise to many evaluation datasets and models over the decades. Recently the emergence of Large Language Models (LLM) raises much hope in improving accuracy. In this work, we evaluate word sense disambiguation capabilities of four LLMs: OpenAI's ChatGPT-3.5, Mistral's 7b parameter model, Meta's Llama 70b, and Google's Gemini Pro. We evaluate many well-established datasets containing a variety of texts and senses on these. After observing the performances of some datasets, we selectively study some failure cases and identify the reasons for failures. We explore human judgments that would correct these failures. Our findings suggest that many failure cases are related to a lack of world knowledge and the reasoning to amalgamate this knowledge rather than the lack of linguistic knowledge. We categorize the judgments so that the next generation of LLMs can improve by incorporating deeper world knowledge and reasoning. We conclude that word sense disambiguation could serve as a guide for probing the reasoning power of LLMs to measure their functional competency. We also list the accuracy of these datasets. We find that on many occasions, accuracy drops to below 70%, which is much less than that of well-performing existing models.

---

## ğŸ“ˆ **Results**

![Comparative Results for WSD Accuracy of different LLMs](images/image.png)

---

## ğŸ”¬ **Reproducing the Results**

To reproduce the experiments, refer to the README files in each folder, where you'll find specific instructions for different datasets and models.


### **Experiment Configurations**

Set your own API keys for Gemini, LLama, Mistral and OpenAI in Google Colab.

---

## ğŸ” **Citations**

If you use this code in your research, please cite:

```bibtex
@inproceedings{kibria2024functional,
  title={On Functional Competence of LLMs for Linguistic Disambiguation},
  author={Kibria, Raihan and Dipta, Sheikh and Adnan, Muhammad},
  booktitle={Proceedings of the 28th Conference on Computational Natural Language Learning},
  pages={143--160},
  year={2024}
}
```

---

## ğŸ—‚ **Project Structure**

```bash
â”œâ”€â”€â”€gemini
â”‚   â”œâ”€â”€â”€coarseWSD-20-gemini
â”‚   â”œâ”€â”€â”€fews
â”‚   â”œâ”€â”€â”€unified_framework
â”‚   â””â”€â”€â”€wic
â”œâ”€â”€â”€images
â”œâ”€â”€â”€llama
â”‚   â”œâ”€â”€â”€coarseWSD-20-Llama
â”‚   â”œâ”€â”€â”€fews
â”‚   â”œâ”€â”€â”€unified_framework
â”‚   â””â”€â”€â”€wic
â”œâ”€â”€â”€mistral
â”‚   â”œâ”€â”€â”€coarseWSD-20-mistral
â”‚   â”œâ”€â”€â”€fews
â”‚   â”œâ”€â”€â”€unified_framework
â”‚   â””â”€â”€â”€wic
â””â”€â”€â”€open-ai
    â”œâ”€â”€â”€coarseWSD-20-openAI
    â”œâ”€â”€â”€fews
    â”œâ”€â”€â”€unified_framework
    â””â”€â”€â”€wic
```

---
